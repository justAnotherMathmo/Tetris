{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetris Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gym_tetris\n",
    "gym_tetris.TetrisEnv\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic? True\n"
     ]
    }
   ],
   "source": [
    "env = gym_tetris.make('Tetris-v0')\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.995\n",
    "MULISTEP_GAMMA = 0.995\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000\n",
    "TARGET_UPDATE = 25\n",
    "NUM_STATES = env.action_space.n\n",
    "MULTISTEP_PARAM = 5\n",
    "MOVEMENT_COST = 0.01\n",
    "LAYER_HISTORY = 4\n",
    "TRAIN_RATE = 4\n",
    "LEARNING_RATE = 5 * 10**-4\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def curr_eps(steps):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps / EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity, good_capacity, bad_capacity):\n",
    "        self.capacity = capacity\n",
    "#         self.good_capacity = good_capacity\n",
    "#         self.bad_capacity = bad_capacity\n",
    "        self.memory = []\n",
    "#         self.good_memories = []\n",
    "#         self.bad_memories = []\n",
    "        self.position = 0\n",
    "#         self.good_position = 0\n",
    "#         self.bad_position = 0\n",
    "        \n",
    "#         self.lower_best_threshold = 15\n",
    "#         self.upper_worst_threshold = -10\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        fleeting_memory = Transition(*args)\n",
    "        self.memory[self.position] = fleeting_memory\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "#         # Good memories\n",
    "#         if (len(self.good_memories) < self.good_capacity) and (fleeting_memory.reward > self.lower_best_threshold):\n",
    "#             self.good_memories.append(None)\n",
    "#         if fleeting_memory.reward > self.lower_best_threshold:\n",
    "#             self.good_memories[self.good_position] = fleeting_memory\n",
    "#             self.good_position = (self.good_position + 1) % self.good_capacity\n",
    "        \n",
    "#         # Bad memories\n",
    "#         if (len(self.bad_memories) < self.bad_capacity) and (fleeting_memory.reward < self.upper_worst_threshold):\n",
    "#             self.bad_memories.append(None)\n",
    "#         if fleeting_memory.reward < self.upper_worst_threshold:\n",
    "#             self.bad_memories[self.bad_position] = fleeting_memory\n",
    "#             self.bad_position = (self.bad_position + 1) % self.bad_capacity        \n",
    "\n",
    "    def sample(self, batch_size, good_fraction=20, bad_fraction=20):\n",
    "#         res = []\n",
    "#         res += random.sample(self.good_memories, min(len(self.good_memories), batch_size // 20))\n",
    "#         res += random.sample(self.bad_memories, min(len(self.bad_memories), batch_size // 20))\n",
    "#         res += random.sample(self.memory, batch_size - 2 * (batch_size // 20))\n",
    "#         return res\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "class BiasedMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.good_memories = []\n",
    "        self.bad_memories = []\n",
    "        self.bias = []\n",
    "        self.bias_sum = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args, bias=1):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.bias.append(None)\n",
    "            self.bias_sum += bias\n",
    "        else:\n",
    "            # Don't add if small bias\n",
    "            if bias < self.bias_sum / len(self.memory) * (curr_eps(steps_done) - EPS_END):\n",
    "                return\n",
    "            self.bias_sum -= self.bias[self.position]\n",
    "            self.bias_sum += bias\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.bias[self.position] = bias\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, biased=True):\n",
    "        if biased:\n",
    "            choice_indices = np.random.choice(len(self.memory), size=batch_size, replace=False, p=np.array(self.bias) / self.bias_sum)\n",
    "            return [self.memory[i] for i in choice_indices]\n",
    "        else:\n",
    "            return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I figure, if we've abstracted away the problem, we can get rid of the convolutional \n",
    "#  layers and make it fully dense...\n",
    "# Will add those in later when we can get the toy model to work, I guess\n",
    "  \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, history=LAYER_HISTORY):\n",
    "        super().__init__()\n",
    "        self.input_layer_width = h * w\n",
    "        \n",
    "        # Encoder section\n",
    "        layer_widths = [\n",
    "            self.input_layer_width,\n",
    "            self.input_layer_width * 2,\n",
    "            self.input_layer_width * 6,\n",
    "            self.input_layer_width * 2,\n",
    "            256,\n",
    "        ]\n",
    "        self.fc1 = nn.Linear(layer_widths[0], layer_widths[1])\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=layer_widths[1])\n",
    "        self.fc2 = nn.Linear(layer_widths[1], layer_widths[2])\n",
    "        self.bn2 = nn.BatchNorm1d(layer_widths[2])\n",
    "        self.fc3 = nn.Linear(layer_widths[2], layer_widths[3])\n",
    "        self.bn3 = nn.BatchNorm1d(layer_widths[3])\n",
    "        \n",
    "        # Value Net\n",
    "        self.value_layer1 = nn.Linear(layer_widths[3], layer_widths[4])\n",
    "        self.vbn = nn.BatchNorm1d(layer_widths[4])\n",
    "        self.value_layer2 = nn.Linear(layer_widths[4], 1)\n",
    "        \n",
    "        # Advantage Net\n",
    "        self.advantage_layer1 = nn.Linear(layer_widths[3], layer_widths[4])\n",
    "        self.abn = nn.BatchNorm1d(layer_widths[4])\n",
    "        self.advantage_layer2 = nn.Linear(layer_widths[4], NUM_STATES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = F.relu(self.bn1(self.fc1(x.view(x.size(0), -1))))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        \n",
    "        value = F.relu(self.vbn(self.value_layer1(x)))\n",
    "        value = self.value_layer2(value)\n",
    "        \n",
    "        advg = F.relu(self.abn(self.advantage_layer1(x)))\n",
    "        advg = self.advantage_layer2(advg)\n",
    "        \n",
    "        return  value.expand(-1, NUM_STATES) + (advg - advg.mean(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_state(state_var):\n",
    "    \"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\n",
    "    greyscale  = np.sum(state_var, axis=2) / (3 * 255)\n",
    "    return greyscale\n",
    "\n",
    "def compress_board(state):\n",
    "    \"\"\"Assumes board greyscale\"\"\"\n",
    "    small_board = state[10:423:20, 20:213:20]\n",
    "    next_piece = state[180:241:20, 235:296:20]\n",
    "    return small_board, next_piece\n",
    "\n",
    "def combine_board_and_piece(board, piece):\n",
    "    screen = np.where(board > 0, 1.0, 0.0)\n",
    "    bool_piece = np.vstack([np.where(piece > 0, 1.0, 0.0), [0.0, 0.0, 0.0, 0.0]]).reshape([2,10])\n",
    "    return np.vstack([bool_piece, screen])\n",
    "\n",
    "def get_screen(screen=None, human=False):\n",
    "    if screen is None and not human:\n",
    "        screen = env.render(mode='rgb_array')\n",
    "    if human:\n",
    "        bla = env.render()\n",
    "        screen = env.env.screen\n",
    "        \n",
    "    # Turn greyscale\n",
    "    screen = clean_state(screen)\n",
    "    \n",
    "    # Compress\n",
    "    screen, piece = compress_board(screen)\n",
    "    screen = combine_board_and_piece(screen, piece)\n",
    "    \n",
    "    # Resize and add a batch dimension (BCHW)\n",
    "    tensor = torch.from_numpy(screen).unsqueeze(0).unsqueeze(0)\n",
    "    # Push to floats on GPU\n",
    "    return tensor.type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fell back to creating a new net...\n"
     ]
    }
   ],
   "source": [
    "load_net_prefix = './models/tetrisBotDuelOONLY1v'\n",
    "load_net_number = 0\n",
    "net_to_load = f'{load_net_prefix}{load_net_number}'\n",
    "try:\n",
    "    policy_net = torch.load(net_to_load)\n",
    "    policy_net.eval()\n",
    "    target_net = torch.load(net_to_load)\n",
    "    target_net.eval()\n",
    "    print(f'{net_to_load} loaded...')\n",
    "except:\n",
    "    policy_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    print(f'Fell back to creating a new net...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(1000000, 10000, 10000)\n",
    "\n",
    "def select_action(state, deterministic=False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = curr_eps(steps_done)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold and not deterministic:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net.eval()(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(NUM_STATES)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "lines_cleared = []\n",
    "eps_values = []\n",
    "game_rewards = []\n",
    "\n",
    "def plot_durations(save=None):\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.plot(np.array(lines_cleared) * 200)\n",
    "    plt.plot(np.array(eps_values) * 500)\n",
    "    plt.plot(np.array(game_rewards))\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    if save is not None:\n",
    "        fig.savefig(save, bbox_inches='tight')\n",
    "        \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single(state, action, next_state, reward):\n",
    "    return _compute_loss(state, action, next_state, reward, batch_size=1)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = _compute_loss(state_batch, action_batch, next_state_batch, reward_batch)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "def _compute_loss(_state, _action, _next_state, _reward, batch_size=BATCH_SIZE):\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(_state).gather(1, _action)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "#     next_state_values = target_net(_next_state).max(1)[0].detach()\n",
    "    \n",
    "#     Double Q learning:\n",
    "    next_state_values = target_net(_next_state)[0][policy_net(_next_state).argmax(1)[0]].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + _reward\n",
    "\n",
    "    # Compute Huber loss\n",
    "    return F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [False]\n",
    "def did_piece_fall(env):\n",
    "    this_result = results.pop()\n",
    "    next_result = (env.unwrapped.game.falling_piece is None)\n",
    "    results.append(next_result)\n",
    "    return this_result   \n",
    "        \n",
    "\n",
    "def create_reward(this_env, block_placed, action, is_done, info,\n",
    "                  old_height, old_lines, hole_count=0, hole_towers=0,\n",
    "                  include_height=True, include_score=True, include_holes=False, include_towers=False):\n",
    "    \"\"\"Assumes states are 21 x 10\"\"\"\n",
    "    if not block_placed:\n",
    "        # Punish a little for doing something that isn't the empty move, or down\n",
    "        if action == 0:\n",
    "            return 0\n",
    "#         if action == 3:\n",
    "#             return MOVEMENT_COST\n",
    "        else:\n",
    "            return -MOVEMENT_COST\n",
    "    if is_done:\n",
    "        return -10.0\n",
    "    \n",
    "    total_reward = 0\n",
    "    if include_height:\n",
    "        if info['height'] > old_height: \n",
    "            # Punish a little more the closer you are to the top\n",
    "            total_reward += (1 + info['height'] / 10) * (old_height - info['height']) /3\n",
    "    \n",
    "    line_diff = this_env.unwrapped.game.complete_lines - old_lines\n",
    "    if include_score and line_diff != 0:\n",
    "        total_reward += 20 * 2 ** (line_diff)\n",
    "    \n",
    "    if include_holes:\n",
    "        total_reward -= hole_count * 1.5\n",
    "    if include_towers:\n",
    "        total_reward -= hole_towers\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def create_reward2(num_steps, done):\n",
    "    if done:\n",
    "        return -5\n",
    "    else:\n",
    "        return min(num_steps / 3000., 1)\n",
    "\n",
    "def num_holes(state):\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    return np.sum(np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)[1:, :])\n",
    "\n",
    "def num_holy_towers(state):\n",
    "    \"\"\"This is a fucking work of art\"\"\"\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    mask = np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)\n",
    "    return np.sum(np.where(mask, flat_state.cumsum(axis=0), 0))\n",
    "\n",
    "def train(num_episodes = 1000, human=False): \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        height, lines = 0, 0\n",
    "        env.reset()\n",
    "        last_state = get_screen(human=human)\n",
    "        state = get_screen(human=human)\n",
    "        hole_count = 0 \n",
    "        hole_reward = 0\n",
    "        tower_count = 0 \n",
    "        tower_reward = 0\n",
    "        total_reward = 0 \n",
    "        if not human:\n",
    "            state_array = [last_state] * MULTISTEP_PARAM\n",
    "            reward_array = [0] * MULTISTEP_PARAM\n",
    "            \n",
    "            reward_sum = 0\n",
    "            array_pos = 0\n",
    "            next_array_pos = 1\n",
    "            warmup = 1\n",
    "        for t in count():\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_action(state, deterministic=human)\n",
    "            # Can only perform an action once every three frames anyway...\n",
    "            state, _, done, info = env.step(action.item())\n",
    "            piece_fell = did_piece_fall(env)\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "\n",
    "            # Observe new state\n",
    "            state = get_screen(state, human)\n",
    "            \n",
    "            if not human:\n",
    "                state_array[array_pos] = state\n",
    "                \n",
    "                # Rewards\n",
    "                if piece_fell:\n",
    "                    # Holes and towers don't quite work - there's actually an extra action to do beforehand... =(\n",
    "                    # Holes\n",
    "                    new_holes = num_holes(last_state)\n",
    "                    holes_reward = new_holes - hole_count\n",
    "                    hole_count = new_holes\n",
    "                    # Towers\n",
    "                    new_towers = num_holy_towers(last_state)\n",
    "                    tower_reward = new_towers - tower_count\n",
    "                    tower_count = new_towers\n",
    "                    \n",
    "#                     print(np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0))\n",
    "                else:\n",
    "                    holes_reward = 0\n",
    "                    tower_reward = 0\n",
    "                \n",
    "                reward_single = create_reward(env, piece_fell, action, done, info, height, lines, holes_reward, tower_reward)\n",
    "                total_reward += reward_single\n",
    "#                 reward_single = create_reward2(t, done)\n",
    "                reward_sum = (MULISTEP_GAMMA * reward_sum) + reward_single - (MULISTEP_GAMMA ** MULTISTEP_PARAM) * reward_array[array_pos]\n",
    "                reward_array[array_pos] = reward_single\n",
    "                reward_sum = torch.tensor([reward_sum], device=device).type(torch.float)\n",
    "                \n",
    "                # Store the transition in memory\n",
    "                if warmup > MULTISTEP_PARAM:\n",
    "#                     with torch.no_grad():\n",
    "#                         loss = compute_loss_single(state_array[next_array_pos], action, state, reward_sum) ** ((1 - curr_eps(steps_done)) / 2 + 0.05)\n",
    "#                     memory.push(state_array[next_array_pos], action, state, reward_sum, bias=np.array([loss.cpu()])[0])\n",
    "                    memory.push(state_array[next_array_pos], action, state, reward_sum)\n",
    "                \n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                if (warmup + 1) % TRAIN_RATE == 0:\n",
    "                    optimize_model()\n",
    "                if done:\n",
    "                    episode_durations.append(t + 1)\n",
    "                    game_rewards.append(total_reward)\n",
    "                    lines_cleared.append(lines)\n",
    "                    eps_values.append(curr_eps(steps_done))\n",
    "                    plot_durations('latestConv.png')\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Set up params for next cycle\n",
    "            height = info['height']\n",
    "            lines = env.unwrapped.game.complete_lines\n",
    "            last_state = state\n",
    "            if not human:\n",
    "                array_pos = (array_pos + 1) % MULTISTEP_PARAM\n",
    "                next_array_pos = (next_array_pos + 1) % MULTISTEP_PARAM\n",
    "                warmup += 1\n",
    "            \n",
    "        if not human:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "def watch_model(rounds=1000):\n",
    "    with torch.no_grad():\n",
    "        train(rounds, human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-92da7837eaad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{load_net_prefix}{idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f035ef0c2f90>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mreward_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMULISTEP_GAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward_sum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward_single\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMULISTEP_GAMMA\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mMULTISTEP_PARAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marray_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mreward_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marray_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_single\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mreward_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m# Store the transition in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "while True:\n",
    "    train(3000)\n",
    "    torch.save(policy_net, f'{load_net_prefix}{idx}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
       "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [1., 0., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
       "          [1., 1., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 1., 1., 1., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_screen()#policy_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, f'{load_net_prefix}{idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_screen()\n",
    "x = F.relu(policy_net.fc1(x.view(x.size(0), -1)))\n",
    "x = F.relu(policy_net.fc2(x))\n",
    "x = F.relu(policy_net.fc3(x))\n",
    "\n",
    "value = F.relu(policy_net.value_layer1(x))\n",
    "value = policy_net.value_layer2(value)\n",
    "\n",
    "advg = F.relu(policy_net.advantage_layer1(x))\n",
    "advg = policy_net.advantage_layer2(advg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gym.envs.classic_control.rendering.SimpleImageViewer object at 0x7f6c8414b048>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ae85c8d89772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-38d41e812477>\u001b[0m in \u001b[0;36mwatch_model\u001b[0;34m(rounds)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-38d41e812477>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Can only perform an action once every three frames anyway...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mpiece_fell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdid_piece_fall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# if this step has passed the max number, set the episode to done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'height'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36mscreen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;34m\"\"\"Return the screen as a NumPy array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_screen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pygame/surfarray.py\u001b[0m in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    127\u001b[0m     method).\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnumpysf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpixels3d\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pygame/_numpysurfarray.py\u001b[0m in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0msurface_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "watch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0765313099202285"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_eps(steps_done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
