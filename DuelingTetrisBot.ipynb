{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetris Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gym_tetris\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_tetris.make('Tetris-v0')\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.99\n",
    "MULISTEP_GAMMA = 0.99\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 2000000\n",
    "TARGET_UPDATE = 25\n",
    "NUM_STATES = env.action_space.n\n",
    "MULTISTEP_PARAM = 5\n",
    "MOVEMENT_COST = 0.01\n",
    "LAYER_HISTORY = 4\n",
    "TRAIN_RATE = 4\n",
    "LEARNING_RATE = 10**-4\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def curr_eps(steps):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps / EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity, good_capacity, bad_capacity):\n",
    "        self.capacity = capacity\n",
    "#         self.good_capacity = good_capacity\n",
    "#         self.bad_capacity = bad_capacity\n",
    "        self.memory = []\n",
    "#         self.good_memories = []\n",
    "#         self.bad_memories = []\n",
    "        self.position = 0\n",
    "#         self.good_position = 0\n",
    "#         self.bad_position = 0\n",
    "        \n",
    "#         self.lower_best_threshold = 15\n",
    "#         self.upper_worst_threshold = -10\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        fleeting_memory = Transition(*args)\n",
    "        self.memory[self.position] = fleeting_memory\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "#         # Good memories\n",
    "#         if (len(self.good_memories) < self.good_capacity) and (fleeting_memory.reward > self.lower_best_threshold):\n",
    "#             self.good_memories.append(None)\n",
    "#         if fleeting_memory.reward > self.lower_best_threshold:\n",
    "#             self.good_memories[self.good_position] = fleeting_memory\n",
    "#             self.good_position = (self.good_position + 1) % self.good_capacity\n",
    "        \n",
    "#         # Bad memories\n",
    "#         if (len(self.bad_memories) < self.bad_capacity) and (fleeting_memory.reward < self.upper_worst_threshold):\n",
    "#             self.bad_memories.append(None)\n",
    "#         if fleeting_memory.reward < self.upper_worst_threshold:\n",
    "#             self.bad_memories[self.bad_position] = fleeting_memory\n",
    "#             self.bad_position = (self.bad_position + 1) % self.bad_capacity        \n",
    "\n",
    "    def sample(self, batch_size, good_fraction=20, bad_fraction=20):\n",
    "#         res = []\n",
    "#         res += random.sample(self.good_memories, min(len(self.good_memories), batch_size // 20))\n",
    "#         res += random.sample(self.bad_memories, min(len(self.bad_memories), batch_size // 20))\n",
    "#         res += random.sample(self.memory, batch_size - 2 * (batch_size // 20))\n",
    "#         return res\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "class BiasedMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.good_memories = []\n",
    "        self.bad_memories = []\n",
    "        self.bias = []\n",
    "        self.bias_sum = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args, bias=1):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.bias.append(None)\n",
    "            self.bias_sum += bias\n",
    "        else:\n",
    "            # Don't add if small bias\n",
    "            if bias < self.bias_sum / len(self.memory) * (curr_eps(steps_done) - EPS_END):\n",
    "                return\n",
    "            self.bias_sum -= self.bias[self.position]\n",
    "            self.bias_sum += bias\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.bias[self.position] = bias\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, biased=True):\n",
    "        if biased:\n",
    "            choice_indices = np.random.choice(len(self.memory), size=batch_size, replace=False, p=np.array(self.bias) / self.bias_sum)\n",
    "            return [self.memory[i] for i in choice_indices]\n",
    "        else:\n",
    "            return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I figure, if we've abstracted away the problem, we can get rid of the convolutional \n",
    "#  layers and make it fully dense...\n",
    "# Will add those in later when we can get the toy model to work, I guess\n",
    "  \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, history=LAYER_HISTORY):\n",
    "        super().__init__()\n",
    "        self.input_layer_width = h * w\n",
    "        \n",
    "        # Encoder section\n",
    "        self.fc1 = nn.Linear(self.input_layer_width, self.input_layer_width * 3)\n",
    "        self.fc2 = nn.Linear(self.input_layer_width * 3, self.input_layer_width * 8)\n",
    "        self.fc3 = nn.Linear(self.input_layer_width * 8, self.input_layer_width * 3)        \n",
    "        \n",
    "        # Value Net\n",
    "        self.value_layer1 = nn.Linear(self.input_layer_width * 3, 512)\n",
    "        self.value_layer2 = nn.Linear(512, 1)\n",
    "        \n",
    "        # Advantage Net\n",
    "        self.advantage_layer1 = nn.Linear(self.input_layer_width * 3, 512)\n",
    "        self.advantage_layer2 = nn.Linear(512, NUM_STATES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        value = F.relu(self.value_layer1(x))\n",
    "        value = self.value_layer2(value)\n",
    "        \n",
    "        advg = F.relu(self.advantage_layer1(x))\n",
    "        advg = self.advantage_layer2(advg)\n",
    "        \n",
    "        return  value.expand(-1, NUM_STATES) + (advg - advg.mean(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_state(state_var):\n",
    "    \"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\n",
    "    greyscale  = np.sum(state_var, axis=2) / (3 * 255)\n",
    "    return greyscale\n",
    "\n",
    "def compress_board(state):\n",
    "    \"\"\"Assumes board greyscale\"\"\"\n",
    "    small_board = state[10:423:20, 20:213:20]\n",
    "    next_piece = state[180:241:20, 235:296:20]\n",
    "    return small_board, next_piece\n",
    "\n",
    "def combine_board_and_piece(board, piece):\n",
    "    return board\n",
    "\n",
    "def get_screen(screen=None, human=False):\n",
    "    if screen is None and not human:\n",
    "        screen = env.render(mode='rgb_array')\n",
    "    if human:\n",
    "        bla = env.render()\n",
    "        screen = env.env.screen\n",
    "        \n",
    "    # Turn greyscale\n",
    "    screen = clean_state(screen)\n",
    "    \n",
    "    # Compress\n",
    "    screen, piece = compress_board(screen)\n",
    "    screen = combine_board_and_piece(screen, piece)\n",
    "    \n",
    "    # Resize and add a batch dimension (BCHW)\n",
    "    tensor = torch.from_numpy(screen).unsqueeze(0).unsqueeze(0)\n",
    "    # Push to floats on GPU\n",
    "    return tensor.type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fell back to creating a new net...\n"
     ]
    }
   ],
   "source": [
    "load_net_prefix = './models/tetrisBotDuel3v'\n",
    "load_net_number = 0\n",
    "net_to_load = f'{load_net_prefix}{load_net_number}'\n",
    "try:\n",
    "    policy_net = torch.load(net_to_load)\n",
    "    policy_net.eval()\n",
    "    target_net = torch.load(net_to_load)\n",
    "    target_net.eval()\n",
    "    print(f'{net_to_load} loaded...')\n",
    "except:\n",
    "    policy_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    print(f'Fell back to creating a new net...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(1000000, 10000, 10000)\n",
    "\n",
    "def select_action(state, deterministic=False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = curr_eps(steps_done)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold and not deterministic:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(NUM_STATES)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "lines_cleared = []\n",
    "eps_values = []\n",
    "\n",
    "def plot_durations(save=None):\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.plot(np.array(lines_cleared) * 200)\n",
    "    plt.plot(np.array(eps_values) * 500)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    if save is not None:\n",
    "        fig.savefig(save, bbox_inches='tight')\n",
    "        \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single(state, action, next_state, reward):\n",
    "    return _compute_loss(state, action, next_state, reward, batch_size=1)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = _compute_loss(state_batch, action_batch, next_state_batch, reward_batch)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "def _compute_loss(_state, _action, _next_state, _reward, batch_size=BATCH_SIZE):\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(_state).gather(1, _action)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "#     next_state_values = target_net(_next_state).max(1)[0].detach()\n",
    "    \n",
    "#     Double Q learning:\n",
    "    next_state_values = target_net(_next_state)[0][policy_net(_next_state).argmax(1)[0]].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + _reward\n",
    "\n",
    "    # Compute Huber loss\n",
    "    return F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [False]\n",
    "def did_piece_fall(env):\n",
    "    this_result = results.pop()\n",
    "    next_result = (env.unwrapped.game.falling_piece is None)\n",
    "    results.append(next_result)\n",
    "    return this_result   \n",
    "        \n",
    "\n",
    "def create_reward(this_env, block_placed, action, is_done, info,\n",
    "                  old_height, old_lines, hole_count=0, hole_towers=0,\n",
    "                  include_height=True, include_score=True, include_holes=False, include_towers=False):\n",
    "    \"\"\"Assumes states are 21 x 10\"\"\"\n",
    "    if not block_placed:\n",
    "        # Punish a little for doing something that isn't the empty move, or down\n",
    "        if action == 0:\n",
    "            return 0\n",
    "#         if action == 3:\n",
    "#             return MOVEMENT_COST\n",
    "        else:\n",
    "            return -MOVEMENT_COST\n",
    "    if is_done:\n",
    "        return -10.0\n",
    "    \n",
    "    total_reward = 0\n",
    "    if include_height:\n",
    "        if info['height'] > old_height: \n",
    "            # Punish a little more the closer you are to the top\n",
    "            total_reward += (1 + info['height'] / 10) * (old_height - info['height']) /3\n",
    "    \n",
    "    line_diff = this_env.unwrapped.game.complete_lines - old_lines\n",
    "    if include_score and line_diff != 0:\n",
    "        total_reward += 20 * 2 ** (line_diff)\n",
    "    \n",
    "    if include_holes:\n",
    "        total_reward -= hole_count * 1.5\n",
    "    if include_towers:\n",
    "        total_reward -= hole_towers\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def create_reward2(num_steps, done):\n",
    "    if done:\n",
    "        return -5\n",
    "    else:\n",
    "        return min(num_steps / 3000., 1)\n",
    "\n",
    "def num_holes(state):\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    return np.sum(np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)[1:, :])\n",
    "\n",
    "def num_holy_towers(state):\n",
    "    \"\"\"This is a fucking work of art\"\"\"\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    mask = np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)\n",
    "    return np.sum(np.where(mask, flat_state.cumsum(axis=0), 0))\n",
    "\n",
    "def train(num_episodes = 1000, human=False): \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        height, lines = 0, 0\n",
    "        env.reset()\n",
    "        last_state = get_screen(human=human)\n",
    "        state = get_screen(human=human)\n",
    "        hole_count = 0 \n",
    "        hole_reward = 0\n",
    "        tower_count = 0 \n",
    "        tower_reward = 0\n",
    "        if not human:\n",
    "            state_array = [last_state] * MULTISTEP_PARAM\n",
    "            reward_array = [0] * MULTISTEP_PARAM\n",
    "            \n",
    "            reward_sum = 0\n",
    "            array_pos = 0\n",
    "            next_array_pos = 1\n",
    "            warmup = 1\n",
    "        for t in count():\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_action(state, deterministic=human)\n",
    "            # Can only perform an action once every three frames anyway...\n",
    "            state, _, done, info = env.step(action.item())\n",
    "            piece_fell = did_piece_fall(env)\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "\n",
    "            # Observe new state\n",
    "            state = get_screen(state, human)\n",
    "            \n",
    "            if not human:\n",
    "                state_array[array_pos] = state\n",
    "                \n",
    "                # Rewards\n",
    "                if piece_fell:\n",
    "                    # Holes and towers don't quite work - there's actually an extra action to do beforehand... =(\n",
    "                    # Holes\n",
    "                    new_holes = num_holes(last_state)\n",
    "                    holes_reward = new_holes - hole_count\n",
    "                    hole_count = new_holes\n",
    "                    # Towers\n",
    "                    new_towers = num_holy_towers(last_state)\n",
    "                    tower_reward = new_towers - tower_count\n",
    "                    tower_count = new_towers\n",
    "                    \n",
    "#                     print(np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0))\n",
    "                else:\n",
    "                    holes_reward = 0\n",
    "                    tower_reward = 0\n",
    "                \n",
    "                reward_single = create_reward(env, piece_fell, action, done, info, height, lines, holes_reward, tower_reward)\n",
    "#                 reward_single = create_reward2(t, done)\n",
    "                reward_sum = (MULISTEP_GAMMA * reward_sum) + reward_single - (MULISTEP_GAMMA ** MULTISTEP_PARAM) * reward_array[array_pos]\n",
    "                reward_array[array_pos] = reward_single\n",
    "                reward_sum = torch.tensor([reward_sum], device=device).type(torch.float)\n",
    "                \n",
    "                # Store the transition in memory\n",
    "                if warmup > MULTISTEP_PARAM:\n",
    "#                     with torch.no_grad():\n",
    "#                         loss = compute_loss_single(state_array[next_array_pos], action, state, reward_sum) ** ((1 - curr_eps(steps_done)) / 2 + 0.05)\n",
    "#                     memory.push(state_array[next_array_pos], action, state, reward_sum, bias=np.array([loss.cpu()])[0])\n",
    "                    memory.push(state_array[next_array_pos], action, state, reward_sum)\n",
    "                \n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                if (warmup + 1) % TRAIN_RATE == 0:\n",
    "                    optimize_model()\n",
    "                if done:\n",
    "                    episode_durations.append(t + 1)\n",
    "                    lines_cleared.append(lines)\n",
    "                    eps_values.append(curr_eps(steps_done))\n",
    "                    plot_durations('latest.png')\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Set up params for next cycle\n",
    "            height = info['height']\n",
    "            lines = env.unwrapped.game.complete_lines\n",
    "            last_state = state\n",
    "            if not human:\n",
    "                array_pos = (array_pos + 1) % MULTISTEP_PARAM\n",
    "                next_array_pos = (next_array_pos + 1) % MULTISTEP_PARAM\n",
    "                warmup += 1\n",
    "            \n",
    "        if not human:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "def watch_model(rounds=1000):\n",
    "    with torch.no_grad():\n",
    "        train(rounds, human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0b7b49235773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{load_net_prefix}{idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-72411c6cee20>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Observe new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m(screen, human)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Turn greyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Compress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mclean_state\u001b[0;34m(state_var)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgreyscale\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgreyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2076\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "while True:\n",
    "    train(5000)\n",
    "    torch.save(policy_net, f'{load_net_prefix}{idx}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.3165, -8.9990, -8.8741, -9.0161, -8.9727, -8.9215, -8.9605, -8.8305,\n",
       "         -8.8672, -8.9789, -8.9385, -8.9031]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net(get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, f'{load_net_prefix}{idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_screen()\n",
    "x = F.relu(policy_net.fc1(x.view(x.size(0), -1)))\n",
    "x = F.relu(policy_net.fc2(x))\n",
    "x = F.relu(policy_net.fc3(x))\n",
    "\n",
    "value = F.relu(policy_net.value_layer1(x))\n",
    "value = policy_net.value_layer2(value)\n",
    "\n",
    "advg = F.relu(policy_net.advantage_layer1(x))\n",
    "advg = policy_net.advantage_layer2(advg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gym.envs.classic_control.rendering.SimpleImageViewer object at 0x7fe860a506d8>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ae85c8d89772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6821b307e3a3>\u001b[0m in \u001b[0;36mwatch_model\u001b[0;34m(rounds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6821b307e3a3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Observe new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m(screen, human)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mcaption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 )\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# otherwise the render mode is not supported, raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# draw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36m_wait_vsync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXGetVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXWaitVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "watch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0765313099202285"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_eps(steps_done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
