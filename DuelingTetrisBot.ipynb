{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetris Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gym_tetris\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_tetris.make('Tetris-v0')\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.99\n",
    "MULISTEP_GAMMA = 0.99\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 2000000\n",
    "TARGET_UPDATE = 25\n",
    "NUM_STATES = env.action_space.n\n",
    "MULTISTEP_PARAM = 5\n",
    "MOVEMENT_COST = 0.01\n",
    "LAYER_HISTORY = 4\n",
    "TRAIN_RATE = 4\n",
    "LEARNING_RATE = 10**-4\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def curr_eps(steps):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps / EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity, good_capacity, bad_capacity):\n",
    "        self.capacity = capacity\n",
    "#         self.good_capacity = good_capacity\n",
    "#         self.bad_capacity = bad_capacity\n",
    "        self.memory = []\n",
    "#         self.good_memories = []\n",
    "#         self.bad_memories = []\n",
    "        self.position = 0\n",
    "#         self.good_position = 0\n",
    "#         self.bad_position = 0\n",
    "        \n",
    "#         self.lower_best_threshold = 15\n",
    "#         self.upper_worst_threshold = -10\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        fleeting_memory = Transition(*args)\n",
    "        self.memory[self.position] = fleeting_memory\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "#         # Good memories\n",
    "#         if (len(self.good_memories) < self.good_capacity) and (fleeting_memory.reward > self.lower_best_threshold):\n",
    "#             self.good_memories.append(None)\n",
    "#         if fleeting_memory.reward > self.lower_best_threshold:\n",
    "#             self.good_memories[self.good_position] = fleeting_memory\n",
    "#             self.good_position = (self.good_position + 1) % self.good_capacity\n",
    "        \n",
    "#         # Bad memories\n",
    "#         if (len(self.bad_memories) < self.bad_capacity) and (fleeting_memory.reward < self.upper_worst_threshold):\n",
    "#             self.bad_memories.append(None)\n",
    "#         if fleeting_memory.reward < self.upper_worst_threshold:\n",
    "#             self.bad_memories[self.bad_position] = fleeting_memory\n",
    "#             self.bad_position = (self.bad_position + 1) % self.bad_capacity        \n",
    "\n",
    "    def sample(self, batch_size, good_fraction=20, bad_fraction=20):\n",
    "#         res = []\n",
    "#         res += random.sample(self.good_memories, min(len(self.good_memories), batch_size // 20))\n",
    "#         res += random.sample(self.bad_memories, min(len(self.bad_memories), batch_size // 20))\n",
    "#         res += random.sample(self.memory, batch_size - 2 * (batch_size // 20))\n",
    "#         return res\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "class BiasedMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.good_memories = []\n",
    "        self.bad_memories = []\n",
    "        self.bias = []\n",
    "        self.bias_sum = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args, bias=1):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.bias.append(None)\n",
    "            self.bias_sum += bias\n",
    "        else:\n",
    "            # Don't add if small bias\n",
    "            if bias < self.bias_sum / len(self.memory) * (curr_eps(steps_done) - EPS_END):\n",
    "                return\n",
    "            self.bias_sum -= self.bias[self.position]\n",
    "            self.bias_sum += bias\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.bias[self.position] = bias\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, biased=True):\n",
    "        if biased:\n",
    "            choice_indices = np.random.choice(len(self.memory), size=batch_size, replace=False, p=np.array(self.bias) / self.bias_sum)\n",
    "            return [self.memory[i] for i in choice_indices]\n",
    "        else:\n",
    "            return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I figure, if we've abstracted away the problem, we can get rid of the convolutional \n",
    "#  layers and make it fully dense...\n",
    "# Will add those in later when we can get the toy model to work, I guess\n",
    "class DQNEncoder(nn.Module):\n",
    "    def __init__(self, h, w, output_width, history=LAYER_HISTORY):\n",
    "        super().__init__()\n",
    "        self.input_layer_width = h * w \n",
    "        self.output_width = output_width\n",
    "        self.fc1 = nn.Linear(self.input_layer_width, self.input_layer_width * 3)\n",
    "        self.fc2 = nn.Linear(self.input_layer_width * 3, self.input_layer_width * 8)\n",
    "        self.fc3 = nn.Linear(self.input_layer_width * 8, self.input_layer_width * 3)\n",
    "        self.output_layer = nn.Linear(self.input_layer_width * 3, output_width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class DQNValueNet(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.input_width = encoder.output_width\n",
    "        self.output_layer = nn.Linear(self.input_width, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.encoder(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class DQNAdvantageNet(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.input_width = encoder.output_width\n",
    "        self.output_layer = nn.Linear(self.input_width, NUM_POSSIBLE_ACTIONS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.encoder(x))\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, history=LAYER_HISTORY):\n",
    "        super().__init__()\n",
    "        self.input_layer_width = h * w\n",
    "        \n",
    "        # Encoder section\n",
    "        self.fc1 = nn.Linear(self.input_layer_width, self.input_layer_width * 3)\n",
    "        self.fc2 = nn.Linear(self.input_layer_width * 3, self.input_layer_width * 8)\n",
    "        self.fc3 = nn.Linear(self.input_layer_width * 8, self.input_layer_width * 3)        \n",
    "        \n",
    "        # Value Net\n",
    "        self.value_layer1 = nn.Linear(self.input_layer_width * 3, 512)\n",
    "        self.value_layer2 = nn.Linear(512, 1)\n",
    "        \n",
    "        # Advantage Net\n",
    "        self.advantage_layer1 = nn.Linear(self.input_layer_width * 3, 512)\n",
    "        self.advantage_layer2 = nn.Linear(512, NUM_STATES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        value = F.relu(self.value_layer1(x))\n",
    "        value = self.value_layer2(value)\n",
    "        \n",
    "        advg = F.relu(self.advantage_layer1(x))\n",
    "        advg = self.advantage_layer2(advg)\n",
    "        \n",
    "        return  value.expand(-1, NUM_STATES) + (advg - advg.mean(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_state(state_var):\n",
    "    \"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\n",
    "    greyscale  = np.sum(state_var, axis=2) / (3 * 255)\n",
    "    return greyscale\n",
    "\n",
    "def compress_board(state):\n",
    "    \"\"\"Assumes board greyscale\"\"\"\n",
    "    small_board = state[10:423:20, 20:213:20]\n",
    "    next_piece = state[180:241:20, 235:296:20]\n",
    "    return small_board, next_piece\n",
    "\n",
    "def combine_board_and_piece(board, piece):\n",
    "    return board\n",
    "\n",
    "def get_screen(screen=None, human=False):\n",
    "    if screen is None and not human:\n",
    "        screen = env.render(mode='rgb_array')\n",
    "    if human:\n",
    "        bla = env.render()\n",
    "        screen = env.env.screen\n",
    "        \n",
    "    # Turn greyscale\n",
    "    screen = clean_state(screen)\n",
    "    \n",
    "    # Compress\n",
    "    screen, piece = compress_board(screen)\n",
    "    screen = combine_board_and_piece(screen, piece)\n",
    "    \n",
    "    # Resize and add a batch dimension (BCHW)\n",
    "    tensor = torch.from_numpy(screen).unsqueeze(0).unsqueeze(0)\n",
    "    # Push to floats on GPU\n",
    "    return tensor.type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fell back to creating a new net...\n"
     ]
    }
   ],
   "source": [
    "load_net_prefix = './models/tetrisBotDuel2v'\n",
    "load_net_number = 0\n",
    "net_to_load = f'{load_net_prefix}{load_net_number}'\n",
    "try:\n",
    "    policy_net = torch.load(net_to_load)\n",
    "    policy_net.eval()\n",
    "    target_net = torch.load(net_to_load)\n",
    "    target_net.eval()\n",
    "    print(f'{net_to_load} loaded...')\n",
    "except:\n",
    "    policy_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    print(f'Fell back to creating a new net...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(1000000, 10000, 10000)\n",
    "\n",
    "def select_action(state, deterministic=False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = curr_eps(steps_done)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold and not deterministic:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(NUM_STATES)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "lines_cleared = []\n",
    "eps_values = []\n",
    "\n",
    "def plot_durations(save=None):\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.plot(np.array(lines_cleared) * 200)\n",
    "    plt.plot(np.array(eps_values) * 500)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    if save is not None:\n",
    "        fig.savefig(save, bbox_inches='tight')\n",
    "        \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single(state, action, next_state, reward):\n",
    "    return _compute_loss(state, action, next_state, reward, batch_size=1)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = _compute_loss(state_batch, action_batch, next_state_batch, reward_batch)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "def _compute_loss(_state, _action, _next_state, _reward, batch_size=BATCH_SIZE):\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(_state).gather(1, _action)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "#     next_state_values = target_net(_next_state).max(1)[0].detach()\n",
    "    \n",
    "#     Double Q learning:\n",
    "    next_state_values = target_net(_next_state)[0][policy_net(_next_state).argmax(1)[0]].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + _reward\n",
    "\n",
    "    # Compute Huber loss\n",
    "    return F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_piece_fall(env):\n",
    "    return (env.unwrapped.game.falling_piece is None)\n",
    "\n",
    "def create_reward(this_env, block_placed, action, is_done, info,\n",
    "                  old_height, old_lines, hole_count=0, hole_towers=0,\n",
    "                  include_height=True, include_score=True, include_holes=True, include_towers=True):\n",
    "    \"\"\"Assumes states are 21 x 10\"\"\"\n",
    "    if not block_placed:\n",
    "        # Punish a little for doing something that isn't the empty move, or down\n",
    "        if action == 0:\n",
    "            return 0\n",
    "#         if action == 3:\n",
    "#             return MOVEMENT_COST\n",
    "        else:\n",
    "            return -MOVEMENT_COST\n",
    "    if is_done:\n",
    "        return -10.0\n",
    "    \n",
    "    total_reward = 0\n",
    "    if include_height:\n",
    "        if info['height'] > old_height: \n",
    "            # Punish a little more the closer you are to the top\n",
    "            total_reward += (1 + info['height'] / 10) * (old_height - info['height']) /3\n",
    "    \n",
    "    line_diff = this_env.unwrapped.game.complete_lines - old_lines\n",
    "    if include_score and line_diff != 0:\n",
    "        total_reward += 20 * 2 ** (line_diff)\n",
    "    \n",
    "    if include_holes:\n",
    "        total_reward -= hole_count * 1.5\n",
    "    if include_towers:\n",
    "        total_reward -= include_towers\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "def create_reward2(num_steps, done):\n",
    "    if done:\n",
    "        return -5\n",
    "    else:\n",
    "        return min(num_steps / 3000., 1)\n",
    "\n",
    "def num_holes(state):\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    return np.sum(np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)[1:, :])\n",
    "\n",
    "def num_holy_towers(state):\n",
    "    \"\"\"This is a fucking work of art\"\"\"\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    mask = np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)\n",
    "    return np.sum(np.where(mask, flat_state.cumsum(axis=0), 0))\n",
    "\n",
    "def train(num_episodes = 1000, human=False): \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        height, lines = 0, 0\n",
    "        env.reset()\n",
    "        last_state = get_screen(human=human)\n",
    "        state = get_screen(human=human)\n",
    "        hole_count = 0 \n",
    "        hole_reward = 0\n",
    "        tower_count = 0 \n",
    "        tower_reward = 0\n",
    "        if not human:\n",
    "            state_array = [last_state] * MULTISTEP_PARAM\n",
    "            reward_array = [0] * MULTISTEP_PARAM\n",
    "            \n",
    "            reward_sum = 0\n",
    "            array_pos = 0\n",
    "            next_array_pos = 1\n",
    "            warmup = 1\n",
    "        for t in count():\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_action(state, deterministic=human)\n",
    "            # Can only perform an action once every three frames anyway...\n",
    "            state, _, done, info = env.step(action.item())\n",
    "            piece_fell = did_piece_fall(env)\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "\n",
    "            # Observe new state\n",
    "            state = get_screen(state, human)\n",
    "            \n",
    "            if not human:\n",
    "                state_array[array_pos] = state\n",
    "                \n",
    "                # Rewards\n",
    "                if piece_fell:\n",
    "                    # Holes\n",
    "                    new_holes = num_holes(last_state)\n",
    "                    holes_reward = new_holes - hole_count\n",
    "                    hole_count = new_holes\n",
    "                    # Towers\n",
    "                    new_towers = num_holy_towers(last_state)\n",
    "                    tower_reward = new_towers - tower_count\n",
    "                    tower_count = new_towers\n",
    "                else:\n",
    "                    holes_reward = 0\n",
    "                    tower_reward = 0\n",
    "                    \n",
    "                reward_single = create_reward(env, piece_fell, action, done, info, height, lines, holes_reward, tower_reward, include_towers=False)\n",
    "#                 reward_single = create_reward2(t, done)\n",
    "                reward_sum = (MULISTEP_GAMMA * reward_sum) + reward_single - (MULISTEP_GAMMA ** MULTISTEP_PARAM) * reward_array[array_pos]\n",
    "                reward_array[array_pos] = reward_single\n",
    "                reward_sum = torch.tensor([reward_sum], device=device).type(torch.float)\n",
    "                \n",
    "                # Store the transition in memory\n",
    "                if warmup > MULTISTEP_PARAM:\n",
    "#                     with torch.no_grad():\n",
    "#                         loss = compute_loss_single(state_array[next_array_pos], action, state, reward_sum) ** ((1 - curr_eps(steps_done)) / 2 + 0.05)\n",
    "#                     memory.push(state_array[next_array_pos], action, state, reward_sum, bias=np.array([loss.cpu()])[0])\n",
    "                    memory.push(state_array[next_array_pos], action, state, reward_sum)\n",
    "                \n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                if (warmup + 1) % TRAIN_RATE == 0:\n",
    "                    optimize_model()\n",
    "                if done:\n",
    "                    episode_durations.append(t + 1)\n",
    "                    lines_cleared.append(lines)\n",
    "                    eps_values.append(curr_eps(steps_done))\n",
    "                    plot_durations('latest.png')\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Set up params for next cycle\n",
    "            height = info['height']\n",
    "            lines = env.unwrapped.game.complete_lines\n",
    "            last_state = state\n",
    "            if not human:\n",
    "                array_pos = (array_pos + 1) % MULTISTEP_PARAM\n",
    "                next_array_pos = (next_array_pos + 1) % MULTISTEP_PARAM\n",
    "                warmup += 1\n",
    "            \n",
    "        if not human:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "def watch_model(rounds=1000):\n",
    "    with torch.no_grad():\n",
    "        train(rounds, human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0b7b49235773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{load_net_prefix}{idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-72411c6cee20>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Observe new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m(screen, human)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Turn greyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Compress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mclean_state\u001b[0;34m(state_var)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgreyscale\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgreyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2076\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "while True:\n",
    "    train(5000)\n",
    "    torch.save(policy_net, f'{load_net_prefix}{idx}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.3165, -8.9990, -8.8741, -9.0161, -8.9727, -8.9215, -8.9605, -8.8305,\n",
       "         -8.8672, -8.9789, -8.9385, -8.9031]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net(get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, f'{load_net_prefix}{idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_screen()\n",
    "x = F.relu(policy_net.fc1(x.view(x.size(0), -1)))\n",
    "x = F.relu(policy_net.fc2(x))\n",
    "x = F.relu(policy_net.fc3(x))\n",
    "\n",
    "value = F.relu(policy_net.value_layer1(x))\n",
    "value = policy_net.value_layer2(value)\n",
    "\n",
    "advg = F.relu(policy_net.advantage_layer1(x))\n",
    "advg = policy_net.advantage_layer2(advg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gym.envs.classic_control.rendering.SimpleImageViewer object at 0x7fe860a506d8>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ae85c8d89772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6821b307e3a3>\u001b[0m in \u001b[0;36mwatch_model\u001b[0;34m(rounds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6821b307e3a3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Observe new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m(screen, human)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mcaption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 )\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# otherwise the render mode is not supported, raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# draw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36m_wait_vsync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXGetVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXWaitVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "watch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0765313099202285"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_eps(steps_done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
