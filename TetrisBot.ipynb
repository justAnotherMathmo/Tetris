{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetris Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gym_tetris\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_tetris.make('Tetris-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.bias = []\n",
    "        self.bias_sum = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args, bias=1):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.bias.append(None)\n",
    "            self.bias_sum += bias\n",
    "        else:\n",
    "            self.bias_sum -= self.bias[self.position]\n",
    "            self.bias_sum += bias\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.bias[self.position] = bias\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, biased=True):\n",
    "        if biased:\n",
    "            choice_indices = np.random.choice(len(self.memory), size=batch_size, replace=False, p=np.array(self.bias) / self.bias_sum)\n",
    "            return [self.memory[i] for i in choice_indices]\n",
    "        else:\n",
    "            return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I figure, if we've abstracted away the problem, we can get rid of the convolutional \n",
    "#  layers and make it fully dense...\n",
    "# Will add those in later when we can get the toy model to work, I guess\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_layer_width = h * w\n",
    "        self.fc1 = nn.Linear(self.input_layer_width, self.input_layer_width * 3)\n",
    "        self.fc2 = nn.Linear(self.input_layer_width * 3, self.input_layer_width * 8)\n",
    "        self.fc3 = nn.Linear(self.input_layer_width * 8, self.input_layer_width * 3)\n",
    "        self.fc4 = nn.Linear(self.input_layer_width * 3, self.input_layer_width)\n",
    "        self.output_layer = nn.Linear(self.input_layer_width, 12)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_state(state_var):\n",
    "    \"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\n",
    "    greyscale  = np.sum(state_var, axis=2) / (3 * 255)\n",
    "    return greyscale\n",
    "\n",
    "def compress_board(state):\n",
    "    \"\"\"Assumes board greyscale\"\"\"\n",
    "    small_board = state[10:423:20, 20:213:20]\n",
    "    next_piece = state[180:241:20, 235:296:20]\n",
    "    return small_board, next_piece\n",
    "\n",
    "def combine_board_and_piece(board, piece):\n",
    "    return board\n",
    "\n",
    "def get_screen(screen=None, human=False):\n",
    "    if screen is None and not human:\n",
    "        screen = env.render(mode='rgb_array')\n",
    "    if human:\n",
    "        bla = env.render()\n",
    "        screen = env.env.screen\n",
    "        \n",
    "    # Turn greyscale\n",
    "    screen = clean_state(screen)\n",
    "    \n",
    "    # Compress\n",
    "    screen, piece = compress_board(screen)\n",
    "    screen = combine_board_and_piece(screen, piece)\n",
    "    \n",
    "    # Resize and add a batch dimension (BCHW)\n",
    "    tensor = torch.from_numpy(screen).unsqueeze(0).unsqueeze(0)\n",
    "    # Push to floats on GPU\n",
    "    return tensor.type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 196\n",
    "GAMMA = 0.97\n",
    "MULISTEP_GAMMA = 0.97\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 2000000\n",
    "TARGET_UPDATE = 50\n",
    "NUM_STATES = env.action_space.n\n",
    "MULTISTEP_PARAM = 5\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fell back to creating a new net...\n"
     ]
    }
   ],
   "source": [
    "net_to_load = './models/tetrisBot5v0'\n",
    "try:\n",
    "    policy_net = torch.load(net_to_load)\n",
    "    policy_net.eval()\n",
    "    target_net = torch.load(net_to_load)\n",
    "    target_net.eval()\n",
    "    print(f'{net_to_load} loaded...')\n",
    "except:\n",
    "    policy_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    print(f'Fell back to creating a new net...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=10**-4)\n",
    "memory = ReplayMemory(100000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def curr_eps(steps):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "\n",
    "def select_action(state, deterministic=False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = curr_eps(steps_done)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold and not deterministic:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(NUM_STATES)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "lines_cleared = []\n",
    "\n",
    "def plot_durations(save=None):\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.plot(np.array(lines_cleared) * 200)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    if save is not None:\n",
    "        fig.savefig(save, bbox_inches='tight')\n",
    "        \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single(state, action, next_state, reward):\n",
    "    return _compute_loss(state, action, next_state, reward, batch_size=1)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE, biased=False)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = _compute_loss(state_batch, action_batch, next_state_batch, reward_batch)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "def _compute_loss(_state, _action, _next_state, _reward, batch_size=BATCH_SIZE):\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(_state).gather(1, _action)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = target_net(_next_state).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + _reward\n",
    "\n",
    "    # Compute Huber loss\n",
    "    return F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_piece_fall(env):\n",
    "    return (env.unwrapped.game.falling_piece is None)\n",
    "\n",
    "def create_reward(this_env, block_placed, action, is_done, info,\n",
    "                  old_height, old_lines, hole_count=0, hole_towers=0,\n",
    "                  include_height=True, include_score=True, include_holes=True, include_towers=True):\n",
    "    \"\"\"Assumes states are 21 x 10\"\"\"\n",
    "    if not block_placed:\n",
    "        # Punish a little for doing something that isn't the empty move\n",
    "        if action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -0.01\n",
    "    if is_done:\n",
    "        return -50.0\n",
    "    \n",
    "    total_reward = 0\n",
    "    if include_height:\n",
    "        if info['height'] > old_height: \n",
    "            # Punish a little more the closer you are to the top\n",
    "            total_reward += (1 + info['height'] / 10) * (old_height - info['height']) /3\n",
    "    \n",
    "    line_diff = this_env.unwrapped.game.complete_lines - old_lines\n",
    "    if include_score and line_diff != 0:\n",
    "        total_reward += 20 * 2 ** (line_diff)\n",
    "    \n",
    "    if include_holes:\n",
    "        total_reward -= hole_count * 1.5\n",
    "    if include_towers:\n",
    "        total_reward -= include_towers\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "def num_holes(state):\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    return np.sum(np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)[1:, :])\n",
    "\n",
    "def num_holy_towers(state):\n",
    "    \"\"\"This is a fucking work of art\"\"\"\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    mask = np.where((np.roll(flat_state, flat_state.shape[1] * i) > 0) & (flat_state == 0), 1, 0)\n",
    "    return np.sum(np.where(mask, flat_state.cumsum(axis=0), 0))\n",
    "\n",
    "def train(num_episodes = 1000, human=False): \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        height, lines = 0, 0\n",
    "        env.reset()\n",
    "        last_state = get_screen(human=human)\n",
    "        state = get_screen(human=human)\n",
    "        hole_count = 0 \n",
    "        hole_reward = 0\n",
    "        tower_count = 0 \n",
    "        tower_reward = 0\n",
    "        if not human:\n",
    "            state_array = np.array([last_state] * MULTISTEP_PARAM)\n",
    "            reward_array = np.array([0] * MULTISTEP_PARAM)\n",
    "            reward_sum = 0\n",
    "            array_pos = 1\n",
    "            last_array_pos = 0\n",
    "            warmup = 1\n",
    "        for t in count():\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_action(state, deterministic=human)\n",
    "            # Can only perform an action once every three frames anyway...\n",
    "            state, _, done, info = env.step(action.item())\n",
    "            piece_fell = did_piece_fall(env)\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "\n",
    "            # Observe new state\n",
    "            state = get_screen(state, human)\n",
    "            state_array[array_pos] = state\n",
    "            \n",
    "            # Start messing with rewards\n",
    "            if piece_fell:\n",
    "                # Holes\n",
    "                new_holes = num_holes(last_state)\n",
    "                holes_reward = new_holes - hole_count\n",
    "                hole_count = new_holes\n",
    "                # Towers\n",
    "                new_towers = num_holy_towers(last_state)\n",
    "                tower_reward = new_towers - tower_count\n",
    "                tower_count = new_towers\n",
    "            else:\n",
    "                holes_reward = 0\n",
    "                tower_reward = 0\n",
    "                \n",
    "            reward_single = create_reward(env, piece_fell, action, done, info, height, lines, holes_reward, tower_reward)\n",
    "            reward_sum = (MULISTEP_GAMMA * reward_sum) + reward_single - (MULISTEP_GAMMA ** MULTISTEP_PARAM) * reward_array[array_pos]\n",
    "            reward_array[array_pos] = reward_single\n",
    "            reward_sum = torch.tensor([reward_sum], device=device).type(torch.float)\n",
    "            \n",
    "            if not human:\n",
    "                # Store the transition in memory\n",
    "                if warmup > MULTISTEP_PARAM:\n",
    "                    with torch.no_grad():\n",
    "                        loss = compute_loss_single(state_array[last_array_pos], action, state, reward_sum) ** ((1 - curr_eps(steps_done)) / 2 + 0.05)\n",
    "                    memory.push(state_array[last_array_pos], action, state, reward_sum, bias=np.array([loss.cpu()])[0])\n",
    "                \n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                optimize_model()\n",
    "                if done:\n",
    "                    episode_durations.append(t + 1)\n",
    "                    lines_cleared.append(lines)\n",
    "                    plot_durations('latest.png')\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Set up params for next cycle\n",
    "            height = info['height']\n",
    "            lines = env.unwrapped.game.complete_lines\n",
    "            last_state = state\n",
    "            array_pos = (array_pos + 1) % MULTISTEP_PARAM\n",
    "            last_array_pos = (last_array_pos + 1) % MULTISTEP_PARAM\n",
    "            warmup += 1\n",
    "            \n",
    "        if not human:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "def watch_model(rounds=1000):\n",
    "    with torch.no_grad():\n",
    "        train(rounds, human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     human = True\n",
    "#     for i_episode in range(1):\n",
    "#         # Initialize the environment and state\n",
    "#         height, lines = 0, 0\n",
    "#         env.reset()\n",
    "#         last_state = get_screen(human=human)\n",
    "#         state = get_screen(human=human)\n",
    "#         hole_count = 0 \n",
    "#         hole_reward = 0\n",
    "#         if not human:\n",
    "#             state_array = [state]\n",
    "#             action_array = []\n",
    "#             reward_array = []\n",
    "    for t in count():\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = select_action(state, deterministic=human)\n",
    "        # Can only perform an action once every three frames anyway...\n",
    "        state, _, done, info = env.step(action.item())\n",
    "        piece_fell = did_piece_fall(env)\n",
    "        if piece_fell:\n",
    "            break\n",
    "        if not done:\n",
    "            state, _, done, info = env.step(0)\n",
    "            piece_fell = (piece_fell or did_piece_fall(env))\n",
    "        if piece_fell:\n",
    "            break\n",
    "        if not done:\n",
    "            state, _, done, info = env.step(0)\n",
    "            piece_fell = (piece_fell or did_piece_fall(env))\n",
    "        if piece_fell:\n",
    "            break\n",
    "\n",
    "        # Observe new state\n",
    "        state = get_screen(state, human)\n",
    "\n",
    "        if piece_fell:\n",
    "            new_holes = num_holes(last_state)\n",
    "            holes_reward = new_holes - hole_count\n",
    "            hole_count = new_holes\n",
    "        else:\n",
    "            holes_reward = 0\n",
    "\n",
    "#                 holy_tower_count = num_holy_towers(state)\n",
    "        reward = create_reward(env, piece_fell, action, done, info, height, lines, holes_reward)\n",
    "        reward = torch.tensor([reward], device=device).type(torch.float)\n",
    "\n",
    "        if not human:\n",
    "            # Store the transition in short term memory\n",
    "            state_array.append(state)\n",
    "            action_array.append(action)\n",
    "            reward_array.append(reward)\n",
    "\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model()\n",
    "            if piece_fell:\n",
    "                reward = 0\n",
    "                for tdiff in range(1, len(action_array) + 1):\n",
    "                    reward = GAMMA * reward + reward_array[-tdiff]\n",
    "\n",
    "                    # bias = compute_loss_single(state_array[-tdiff], action_array[-tdiff], state_array[-1], reward, torch.tensor([tdiff], device=device).type(torch.float)) ** 0.2\n",
    "                    memory.push(state_array[-tdiff], action_array[-tdiff], state_array[-1], reward, torch.tensor([tdiff], device=device).type(torch.float), bias=1)\n",
    "\n",
    "            if done:\n",
    "                episode_durations.append(t + 1)\n",
    "                lines_cleared.append(lines)\n",
    "                plot_durations()\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Set up params for next cycle\n",
    "        height = info['height']\n",
    "        lines = env.unwrapped.game.complete_lines\n",
    "        last_state = state\n",
    "\n",
    "    if not human:\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-915a5e95e554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'./models/tetrisBot5v{idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplot_durations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'latest.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-1246f382f662>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mpiece_fell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdid_piece_fall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mpiece_fell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpiece_fell\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdid_piece_fall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# if this step has passed the max number, set the episode to done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'height'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36mscreen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;34m\"\"\"Return the screen as a NumPy array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_screen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pygame/surfarray.py\u001b[0m in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    127\u001b[0m     method).\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnumpysf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpixels3d\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pygame/_numpysurfarray.py\u001b[0m in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0msurface_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = 23\n",
    "while True:\n",
    "    train(5000)\n",
    "    torch.save(policy_net, f'./models/tetrisBot6v{idx}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ae85c8d89772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-b76b13ce82ec>\u001b[0m in \u001b[0;36mwatch_model\u001b[0;34m(rounds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-1246f382f662>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# Can only perform an action once every three frames anyway...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mpiece_fell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdid_piece_fall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# if this step has passed the max number, set the episode to done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_next_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfalling_piece\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfalling_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36m_draw_next_piece\u001b[0;34m(self, piece)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_screen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_surf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_rect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# draw the \"next\" piece preview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTATUS_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNEXT_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# MARK: private movement methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36m_draw_piece\u001b[0;34m(self, piece, pixel_x, pixel_y)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox_x\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBOXSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBOXSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_next_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36m_draw_box\u001b[0;34m(self, box_x, box_y, color, pixel_x, pixel_y)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# draw the smaller depth perspective effect box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mdepth_rect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpixel_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBOXSIZE\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBOXSIZE\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_screen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLIGHTCOLORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_rect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "watch_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
