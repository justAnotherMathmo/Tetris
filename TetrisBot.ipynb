{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetris Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gym_tetris\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_tetris.make('Tetris-v0')\n",
    "BATCH_SIZE = 196\n",
    "GAMMA = 0.9\n",
    "MULISTEP_GAMMA = 0.98\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 50000000\n",
    "TARGET_UPDATE = 50\n",
    "NUM_STATES = env.action_space.n\n",
    "MULTISTEP_PARAM = 5\n",
    "MOVEMENT_COST = 0.01\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def curr_eps(steps):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps / EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.bias = []\n",
    "        self.bias_sum = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args, bias=1):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.bias.append(None)\n",
    "            self.bias_sum += bias\n",
    "        else:\n",
    "            # Don't add if small bias\n",
    "            if bias < self.bias_sum / len(self.memory) * (curr_eps(steps_done) - EPS_END):\n",
    "                return\n",
    "            self.bias_sum -= self.bias[self.position]\n",
    "            self.bias_sum += bias\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.bias[self.position] = bias\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, biased=True):\n",
    "        if biased:\n",
    "            choice_indices = np.random.choice(len(self.memory), size=batch_size, replace=False, p=np.array(self.bias) / self.bias_sum)\n",
    "            return [self.memory[i] for i in choice_indices]\n",
    "        else:\n",
    "            return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I figure, if we've abstracted away the problem, we can get rid of the convolutional \n",
    "#  layers and make it fully dense...\n",
    "# Will add those in later when we can get the toy model to work, I guess\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_layer_width = h * w\n",
    "        self.fc1 = nn.Linear(self.input_layer_width, self.input_layer_width * 3)\n",
    "        self.fc2 = nn.Linear(self.input_layer_width * 3, self.input_layer_width * 8)\n",
    "        self.fc3 = nn.Linear(self.input_layer_width * 8, self.input_layer_width * 3)\n",
    "        self.fc4 = nn.Linear(self.input_layer_width * 3, self.input_layer_width)\n",
    "        self.output_layer = nn.Linear(self.input_layer_width, 12)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_state(state_var):\n",
    "    \"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\n",
    "    greyscale  = np.sum(state_var, axis=2) / (3 * 255)\n",
    "    return greyscale\n",
    "\n",
    "def compress_board(state):\n",
    "    \"\"\"Assumes board greyscale\"\"\"\n",
    "    small_board = state[10:423:20, 20:213:20]\n",
    "    next_piece = state[180:241:20, 235:296:20]\n",
    "    return small_board, next_piece\n",
    "\n",
    "def combine_board_and_piece(board, piece):\n",
    "    return board\n",
    "\n",
    "def get_screen(screen=None, human=False):\n",
    "    if screen is None and not human:\n",
    "        screen = env.render(mode='rgb_array')\n",
    "    if human:\n",
    "        bla = env.render()\n",
    "        screen = env.env.screen\n",
    "        \n",
    "    # Turn greyscale\n",
    "    screen = clean_state(screen)\n",
    "    \n",
    "    # Compress\n",
    "    screen, piece = compress_board(screen)\n",
    "    screen = combine_board_and_piece(screen, piece)\n",
    "    \n",
    "    # Resize and add a batch dimension (BCHW)\n",
    "    tensor = torch.from_numpy(screen).unsqueeze(0).unsqueeze(0)\n",
    "    # Push to floats on GPU\n",
    "    return tensor.type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fell back to creating a new net...\n"
     ]
    }
   ],
   "source": [
    "load_net_prefix = './models/tetrisBot6v'\n",
    "load_net_number = 0\n",
    "net_to_load = f'{load_net_prefix}{load_net_number}'\n",
    "try:\n",
    "    policy_net = torch.load(net_to_load)\n",
    "    policy_net.eval()\n",
    "    target_net = torch.load(net_to_load)\n",
    "    target_net.eval()\n",
    "    print(f'{net_to_load} loaded...')\n",
    "except:\n",
    "    policy_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    print(f'Fell back to creating a new net...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=10**-4)\n",
    "memory = ReplayMemory(100000)\n",
    "\n",
    "def select_action(state, deterministic=False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = curr_eps(steps_done)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold and not deterministic:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(NUM_STATES)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "lines_cleared = []\n",
    "\n",
    "def plot_durations(save=None):\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.plot(np.array(lines_cleared) * 200)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    if save is not None:\n",
    "        fig.savefig(save, bbox_inches='tight')\n",
    "        \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single(state, action, next_state, reward):\n",
    "    return _compute_loss(state, action, next_state, reward, batch_size=1)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE, biased=False)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = _compute_loss(state_batch, action_batch, next_state_batch, reward_batch)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "def _compute_loss(_state, _action, _next_state, _reward, batch_size=BATCH_SIZE):\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(_state).gather(1, _action)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "#     next_state_values = target_net(_next_state).max(1)[0].detach()\n",
    "    \n",
    "#     Double Q learning:\n",
    "    next_state_values = target_net(get_screen())[0][policy_net(get_screen()).argmax(1)[0]].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + _reward\n",
    "\n",
    "    # Compute Huber loss\n",
    "    return F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_piece_fall(env):\n",
    "    return (env.unwrapped.game.falling_piece is None)\n",
    "\n",
    "def create_reward(this_env, block_placed, action, is_done, info,\n",
    "                  old_height, old_lines, hole_count=0, hole_towers=0,\n",
    "                  include_height=True, include_score=True, include_holes=True, include_towers=True):\n",
    "    \"\"\"Assumes states are 21 x 10\"\"\"\n",
    "    if not block_placed:\n",
    "        # Punish a little for doing something that isn't the empty move\n",
    "        if action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -MOVEMENT_COST\n",
    "    if is_done:\n",
    "        return -50.0\n",
    "    \n",
    "    total_reward = 0\n",
    "    if include_height:\n",
    "        if info['height'] > old_height: \n",
    "            # Punish a little more the closer you are to the top\n",
    "            total_reward += (1 + info['height'] / 10) * (old_height - info['height']) /3\n",
    "    \n",
    "    line_diff = this_env.unwrapped.game.complete_lines - old_lines\n",
    "    if include_score and line_diff != 0:\n",
    "        total_reward += 20 * 2 ** (line_diff)\n",
    "    \n",
    "    if include_holes:\n",
    "        total_reward -= hole_count * 1.5\n",
    "    if include_towers:\n",
    "        total_reward -= include_towers\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "def num_holes(state):\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    return np.sum(np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)[1:, :])\n",
    "\n",
    "def num_holy_towers(state):\n",
    "    \"\"\"This is a fucking work of art\"\"\"\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    mask = np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)\n",
    "    return np.sum(np.where(mask, flat_state.cumsum(axis=0), 0))\n",
    "\n",
    "def train(num_episodes = 1000, human=False): \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        height, lines = 0, 0\n",
    "        env.reset()\n",
    "        last_state = get_screen(human=human)\n",
    "        state = get_screen(human=human)\n",
    "        hole_count = 0 \n",
    "        hole_reward = 0\n",
    "        tower_count = 0 \n",
    "        tower_reward = 0\n",
    "        if not human:\n",
    "            state_array = np.array([last_state] * MULTISTEP_PARAM)\n",
    "            reward_array = np.array([0] * MULTISTEP_PARAM)\n",
    "            reward_sum = 0\n",
    "            array_pos = 0\n",
    "            next_array_pos = 1\n",
    "            warmup = 1\n",
    "        for t in count():\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_action(state, deterministic=human)\n",
    "            # Can only perform an action once every three frames anyway...\n",
    "            state, _, done, info = env.step(action.item())\n",
    "            piece_fell = did_piece_fall(env)\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "\n",
    "            # Observe new state\n",
    "            state = get_screen(state, human)\n",
    "            state_array[array_pos] = state\n",
    "            \n",
    "            # Start messing with rewards\n",
    "            if piece_fell:\n",
    "                # Holes\n",
    "                new_holes = num_holes(last_state)\n",
    "                holes_reward = new_holes - hole_count\n",
    "                hole_count = new_holes\n",
    "                # Towers\n",
    "                new_towers = num_holy_towers(last_state)\n",
    "                tower_reward = new_towers - tower_count\n",
    "                tower_count = new_towers\n",
    "            else:\n",
    "                holes_reward = 0\n",
    "                tower_reward = 0\n",
    "                \n",
    "            reward_single = create_reward(env, piece_fell, action, done, info, height, lines, holes_reward, tower_reward)\n",
    "            reward_sum = (MULISTEP_GAMMA * reward_sum) + reward_single - (MULISTEP_GAMMA ** MULTISTEP_PARAM) * reward_array[array_pos]\n",
    "            reward_array[array_pos] = reward_single\n",
    "            reward_sum = torch.tensor([reward_sum], device=device).type(torch.float)\n",
    "            \n",
    "            if not human:\n",
    "                # Store the transition in memory\n",
    "                if warmup > MULTISTEP_PARAM:\n",
    "                    with torch.no_grad():\n",
    "                        loss = compute_loss_single(state_array[next_array_pos], action, state, reward_sum) ** ((1 - curr_eps(steps_done)) / 2 + 0.05)\n",
    "                    memory.push(state_array[next_array_pos], action, state, reward_sum, bias=np.array([loss.cpu()])[0])\n",
    "                \n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                optimize_model()\n",
    "                if done:\n",
    "                    episode_durations.append(t + 1)\n",
    "                    lines_cleared.append(lines)\n",
    "                    plot_durations('latest.png')\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Set up params for next cycle\n",
    "            height = info['height']\n",
    "            lines = env.unwrapped.game.complete_lines\n",
    "            last_state = state\n",
    "            array_pos = (array_pos + 1) % MULTISTEP_PARAM\n",
    "            next_array_pos = (next_array_pos + 1) % MULTISTEP_PARAM\n",
    "            warmup += 1\n",
    "            \n",
    "        if not human:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "def watch_model(rounds=1000):\n",
    "    with torch.no_grad():\n",
    "        train(rounds, human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "while True:\n",
    "    train(2000)\n",
    "    torch.save(policy_net, f'{load_net_prefix}{idx}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ae85c8d89772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-b76b13ce82ec>\u001b[0m in \u001b[0;36mwatch_model\u001b[0;34m(rounds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-1246f382f662>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# Can only perform an action once every three frames anyway...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mpiece_fell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdid_piece_fall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# if this step has passed the max number, set the episode to done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_next_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfalling_piece\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfalling_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36m_draw_next_piece\u001b[0;34m(self, piece)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_screen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_surf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_rect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# draw the \"next\" piece preview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTATUS_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNEXT_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# MARK: private movement methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36m_draw_piece\u001b[0;34m(self, piece, pixel_x, pixel_y)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox_x\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBOXSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBOXSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_next_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36m_draw_box\u001b[0;34m(self, box_x, box_y, color, pixel_x, pixel_y)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# draw the smaller depth perspective effect box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mdepth_rect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpixel_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBOXSIZE\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBOXSIZE\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_screen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLIGHTCOLORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_rect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "watch_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
