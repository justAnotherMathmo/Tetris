{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net2(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  )\n",
       "  (fc_layers): ModuleList(\n",
       "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=84, out_features=10, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net_old = torch.load('./models/tetrisBot')\n",
    "# net_old.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetris Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gym_tetris\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_tetris.make('Tetris-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.bias = []\n",
    "        self.bias_sum = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args, bias=None):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.bias.append(None)\n",
    "            self.bias_sum += bias\n",
    "        else:\n",
    "            self.bias_sum -= self.bias[self.position]\n",
    "            self.bias_sum += bias\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.bias[self.position] = bias\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, biased=True):\n",
    "        if biased:\n",
    "            choice_indices = np.random.choice(len(self.memory), size=batch_size, replace=False, p=np.array(self.bias) / self.bias_sum)\n",
    "            return [self.memory[i] for i in choice_indices]\n",
    "        else:\n",
    "            return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I figure, if we've abstracted away the problem, we can get rid of the convolutional \n",
    "#  layers and make it fully dense...\n",
    "# Will add those in later when we can get the toy model to work, I guess\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_layer_width = h * w\n",
    "        self.fc1 = nn.Linear(self.input_layer_width, self.input_layer_width * 4)\n",
    "        self.fc2 = nn.Linear(self.input_layer_width * 4, self.input_layer_width * 16)\n",
    "        self.fc3 = nn.Linear(self.input_layer_width * 16, self.input_layer_width * 4)\n",
    "        self.fc4 = nn.Linear(self.input_layer_width * 4, self.input_layer_width)\n",
    "        self.output_layer = nn.Linear(self.input_layer_width, 12)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_state(state_var):\n",
    "    \"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\n",
    "    greyscale  = np.sum(state_var, axis=2) / (3 * 255)\n",
    "    return greyscale\n",
    "\n",
    "def compress_board(state):\n",
    "    \"\"\"Assumes board greyscale\"\"\"\n",
    "    small_board = state[10:423:20, 20:213:20]\n",
    "    next_piece = state[180:241:20, 235:296:20]\n",
    "    return small_board, next_piece\n",
    "\n",
    "def combine_board_and_piece(board, piece):\n",
    "    return board\n",
    "\n",
    "def get_screen(screen=None, human=False):\n",
    "    if screen is None and not human:\n",
    "        screen = env.render(mode='rgb_array')\n",
    "    if human:\n",
    "        bla = env.render()\n",
    "        screen = env.env.screen\n",
    "        \n",
    "    # Turn greyscale\n",
    "    screen = clean_state(screen)\n",
    "    \n",
    "    # Compress\n",
    "    screen, piece = compress_board(screen)\n",
    "    screen = combine_board_and_piece(screen, piece)\n",
    "    \n",
    "    # Resize and add a batch dimension (BCHW)\n",
    "    tensor = torch.from_numpy(screen).unsqueeze(0).unsqueeze(0)\n",
    "    # Push to floats on GPU\n",
    "    return tensor.type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200000\n",
    "TARGET_UPDATE = 15\n",
    "NUM_STATES = env.action_space.n\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width).to(device)\n",
    "target_net = DQN(screen_height, screen_width).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def curr_eps(steps):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps / EPS_DECAY)\n",
    "\n",
    "\n",
    "def select_action(state, deterministic=False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = curr_eps(steps_done)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold and not deterministic:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(NUM_STATES)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "lines_cleared = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.plot(np.array(lines_cleared) * 200)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single(state, action, next_state, reward):\n",
    "    batch = Transition(*zip([state, action, next_state, reward]))\n",
    "    \n",
    "    try:\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    except:\n",
    "        return torch.tensor(0, device=device)\n",
    "    \n",
    "    return _compute_loss(state, action, non_final_mask, non_final_next_states, reward, batch_size=1)\n",
    "    \n",
    "def _compute_loss(_state, _action, _next_state_mask, _next_state, _reward, batch_size=BATCH_SIZE):\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(_state).gather(1, _action)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_values[_next_state_mask] = target_net(_next_state).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + _reward\n",
    "\n",
    "    # Compute Huber loss\n",
    "    return F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = _compute_loss(state_batch, action_batch, non_final_mask, non_final_next_states, reward_batch)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_piece_fall(env):\n",
    "    return (env.unwrapped.game.falling_piece is None)\n",
    "\n",
    "def create_reward(this_env, block_placed, action, is_done, info,\n",
    "                  old_height, old_lines,\n",
    "                  include_height=True, include_score=True):\n",
    "    \"\"\"Assumes states are 21 x 10\"\"\"\n",
    "    if not block_placed:\n",
    "        # Punish a little for doing something that isn't the empty move\n",
    "        if action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -0.001\n",
    "    if is_done:\n",
    "        return -50.0\n",
    "    \n",
    "    total_reward = 0\n",
    "    if include_height:\n",
    "        if info['height'] > old_height: \n",
    "            # Punish a little more the closer you are to the top\n",
    "            total_reward += (1 + info['height'] / 10) * (old_height - info['height']) / 4\n",
    "    \n",
    "    line_diff = this_env.unwrapped.game.complete_lines - old_lines\n",
    "    if include_score and line_diff != 0:\n",
    "        total_reward += 10 * 2 ** (line_diff)\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def train(num_episodes = 1000, human=False):\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        height, lines = 0, 0\n",
    "        env.reset()\n",
    "        last_state = get_screen(human=human)\n",
    "        state = get_screen(human=human)\n",
    "        for t in count():\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_action(state, deterministic=human)\n",
    "            # Can only perform an action once every three frames anyway...\n",
    "            state, _, done, info = env.step(action.item())\n",
    "            piece_fell = did_piece_fall(env)\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "\n",
    "            # Observe new state\n",
    "            state = get_screen(state, human)\n",
    "            if done:\n",
    "                state = None\n",
    "\n",
    "            reward = create_reward(env, piece_fell, action, done, info, height, lines)\n",
    "            reward = torch.tensor([reward], device=device).type(torch.float)\n",
    "            \n",
    "            if not human:\n",
    "                # Store the transition in memory\n",
    "                with torch.no_grad():\n",
    "                    loss = compute_loss_single(last_state, action, state, reward) ** ((1 - curr_eps(steps_done)) / 2)\n",
    "                memory.push(last_state, action, state, reward, bias=np.array([loss.cpu()])[0])\n",
    "\n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                optimize_model()\n",
    "                if done:\n",
    "                    episode_durations.append(t + 1)\n",
    "                    lines_cleared.append(lines)\n",
    "                    plot_durations()\n",
    "                    break\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Set up params for next cycle\n",
    "            height = info['height']\n",
    "            lines = env.unwrapped.game.complete_lines\n",
    "            last_state = state\n",
    "            \n",
    "        if not human:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c1a4781c3b6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'./models/tetrisBot3v{idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5cb6538398b1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Can only perform an action once every three frames anyway...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mpiece_fell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdid_piece_fall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# if this step has passed the max number, set the episode to done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_screen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBGCOLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_next_piece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_piece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfalling_piece\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris.py\u001b[0m in \u001b[0;36m_draw_status\u001b[0;34m(self, score, level)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# draw the score label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mscore_label_surf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_font\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCORE_LABEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXTCOLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mscore_label_rect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_label_surf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mscore_label_rect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSTATUS_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCORE_LABEL_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "while True:\n",
    "    train(1000)\n",
    "    torch.save(policy_net, f'./models/tetrisBot3v{idx}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, f'./models/tetrisBot3v8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_model(rounds=1000):\n",
    "    with torch.no_grad():\n",
    "        train(1000, human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ae85c8d89772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-93d83d631427>\u001b[0m in \u001b[0;36mwatch_model\u001b[0;34m(rounds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-5cb6538398b1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Observe new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m(screen, human)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym_tetris/tetris_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mcaption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 )\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# otherwise the render mode is not supported, raise an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# draw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36m_wait_vsync\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXGetVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mglxext_arb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXWaitVideoSyncSGI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "watch_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
