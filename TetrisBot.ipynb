{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tetris Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gym_tetris\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_tetris.make('Tetris-v0')\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.99\n",
    "MULISTEP_GAMMA = 0.99\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 10000000\n",
    "TARGET_UPDATE = 50\n",
    "NUM_STATES = env.action_space.n\n",
    "MULTISTEP_PARAM = 5\n",
    "MOVEMENT_COST = 0.01\n",
    "LAYER_HISTORY = 4\n",
    "TRAIN_RATE = 4\n",
    "LEARNING_RATE = 10**-4\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def curr_eps(steps):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps / EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity, good_capacity, bad_capacity):\n",
    "        self.capacity = capacity\n",
    "#         self.good_capacity = good_capacity\n",
    "#         self.bad_capacity = bad_capacity\n",
    "        self.memory = []\n",
    "#         self.good_memories = []\n",
    "#         self.bad_memories = []\n",
    "        self.position = 0\n",
    "#         self.good_position = 0\n",
    "#         self.bad_position = 0\n",
    "        \n",
    "#         self.lower_best_threshold = 15\n",
    "#         self.upper_worst_threshold = -10\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        fleeting_memory = Transition(*args)\n",
    "        self.memory[self.position] = fleeting_memory\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "#         # Good memories\n",
    "#         if (len(self.good_memories) < self.good_capacity) and (fleeting_memory.reward > self.lower_best_threshold):\n",
    "#             self.good_memories.append(None)\n",
    "#         if fleeting_memory.reward > self.lower_best_threshold:\n",
    "#             self.good_memories[self.good_position] = fleeting_memory\n",
    "#             self.good_position = (self.good_position + 1) % self.good_capacity\n",
    "        \n",
    "#         # Bad memories\n",
    "#         if (len(self.bad_memories) < self.bad_capacity) and (fleeting_memory.reward < self.upper_worst_threshold):\n",
    "#             self.bad_memories.append(None)\n",
    "#         if fleeting_memory.reward < self.upper_worst_threshold:\n",
    "#             self.bad_memories[self.bad_position] = fleeting_memory\n",
    "#             self.bad_position = (self.bad_position + 1) % self.bad_capacity        \n",
    "\n",
    "    def sample(self, batch_size, good_fraction=20, bad_fraction=20):\n",
    "#         res = []\n",
    "#         res += random.sample(self.good_memories, min(len(self.good_memories), batch_size // 20))\n",
    "#         res += random.sample(self.bad_memories, min(len(self.bad_memories), batch_size // 20))\n",
    "#         res += random.sample(self.memory, batch_size - 2 * (batch_size // 20))\n",
    "#         return res\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "class BiasedMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.good_memories = []\n",
    "        self.bad_memories = []\n",
    "        self.bias = []\n",
    "        self.bias_sum = 0\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args, bias=1):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            self.bias.append(None)\n",
    "            self.bias_sum += bias\n",
    "        else:\n",
    "            # Don't add if small bias\n",
    "            if bias < self.bias_sum / len(self.memory) * (curr_eps(steps_done) - EPS_END):\n",
    "                return\n",
    "            self.bias_sum -= self.bias[self.position]\n",
    "            self.bias_sum += bias\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.bias[self.position] = bias\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size, biased=True):\n",
    "        if biased:\n",
    "            choice_indices = np.random.choice(len(self.memory), size=batch_size, replace=False, p=np.array(self.bias) / self.bias_sum)\n",
    "            return [self.memory[i] for i in choice_indices]\n",
    "        else:\n",
    "            return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I figure, if we've abstracted away the problem, we can get rid of the convolutional \n",
    "#  layers and make it fully dense...\n",
    "# Will add those in later when we can get the toy model to work, I guess\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, history=LAYER_HISTORY):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_layer_width = h * w #* history\n",
    "        self.fc1 = nn.Linear(self.input_layer_width, self.input_layer_width * 3)\n",
    "        self.fc2 = nn.Linear(self.input_layer_width * 3, self.input_layer_width * 8)\n",
    "        self.fc3 = nn.Linear(self.input_layer_width * 8, self.input_layer_width * 3)\n",
    "        self.fc4 = nn.Linear(self.input_layer_width * 3, self.input_layer_width)\n",
    "        self.output_layer = nn.Linear(self.input_layer_width, 12)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_state(state_var):\n",
    "    \"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\n",
    "    greyscale  = np.sum(state_var, axis=2) / (3 * 255)\n",
    "    return greyscale\n",
    "\n",
    "def compress_board(state):\n",
    "    \"\"\"Assumes board greyscale\"\"\"\n",
    "    small_board = state[10:423:20, 20:213:20]\n",
    "    next_piece = state[180:241:20, 235:296:20]\n",
    "    return small_board, next_piece\n",
    "\n",
    "def combine_board_and_piece(board, piece):\n",
    "    return board\n",
    "\n",
    "def get_screen(screen=None, human=False):\n",
    "    if screen is None and not human:\n",
    "        screen = env.render(mode='rgb_array')\n",
    "    if human:\n",
    "        bla = env.render()\n",
    "        screen = env.env.screen\n",
    "        \n",
    "    # Turn greyscale\n",
    "    screen = clean_state(screen)\n",
    "    \n",
    "    # Compress\n",
    "    screen, piece = compress_board(screen)\n",
    "    screen = combine_board_and_piece(screen, piece)\n",
    "    \n",
    "    # Resize and add a batch dimension (BCHW)\n",
    "    tensor = torch.from_numpy(screen).unsqueeze(0).unsqueeze(0)\n",
    "    # Push to floats on GPU\n",
    "    return tensor.type(torch.FloatTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fell back to creating a new net...\n"
     ]
    }
   ],
   "source": [
    "load_net_prefix = './models/tetrisBot7v'\n",
    "load_net_number = 14\n",
    "net_to_load = f'{load_net_prefix}{load_net_number}'\n",
    "try:\n",
    "    policy_net = torch.load(net_to_load)\n",
    "    policy_net.eval()\n",
    "    target_net = torch.load(net_to_load)\n",
    "    target_net.eval()\n",
    "    print(f'{net_to_load} loaded...')\n",
    "except:\n",
    "    policy_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net = DQN(screen_height, screen_width).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    print(f'Fell back to creating a new net...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(1000000, 10000, 10000)\n",
    "\n",
    "def select_action(state, deterministic=False):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = curr_eps(steps_done)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold and not deterministic:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(NUM_STATES)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "lines_cleared = []\n",
    "eps_values = []\n",
    "\n",
    "def plot_durations(save=None):\n",
    "    fig = plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.plot(np.array(lines_cleared) * 200)\n",
    "    plt.plot(np.array(eps_values) * 500)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    if save is not None:\n",
    "        fig.savefig(save, bbox_inches='tight')\n",
    "        \n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "#         display.display(plt.gcf())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_single(state, action, next_state, reward):\n",
    "    return _compute_loss(state, action, next_state, reward, batch_size=1)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = _compute_loss(state_batch, action_batch, next_state_batch, reward_batch)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "def _compute_loss(_state, _action, _next_state, _reward, batch_size=BATCH_SIZE):\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(_state).gather(1, _action)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "#     next_state_values = target_net(_next_state).max(1)[0].detach()\n",
    "    \n",
    "#     Double Q learning:\n",
    "    next_state_values = target_net(_next_state)[0][policy_net(_next_state).argmax(1)[0]].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + _reward\n",
    "\n",
    "    # Compute Huber loss\n",
    "    return F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def did_piece_fall(env):\n",
    "    return (env.unwrapped.game.falling_piece is None)\n",
    "\n",
    "def create_reward(this_env, block_placed, action, is_done, info,\n",
    "                  old_height, old_lines, hole_count=0, hole_towers=0,\n",
    "                  include_height=True, include_score=True, include_holes=True, include_towers=True):\n",
    "    \"\"\"Assumes states are 21 x 10\"\"\"\n",
    "    if not block_placed:\n",
    "        # Punish a little for doing something that isn't the empty move, or down\n",
    "        if action == 0:\n",
    "            return 0\n",
    "#         if action == 3:\n",
    "#             return MOVEMENT_COST\n",
    "        else:\n",
    "            return -MOVEMENT_COST\n",
    "    if is_done:\n",
    "        return -100.0\n",
    "    \n",
    "    total_reward = 0\n",
    "    if include_height:\n",
    "        if info['height'] > old_height: \n",
    "            # Punish a little more the closer you are to the top\n",
    "            total_reward += (1 + info['height'] / 10) * (old_height - info['height']) /3\n",
    "    \n",
    "    line_diff = this_env.unwrapped.game.complete_lines - old_lines\n",
    "    if include_score and line_diff != 0:\n",
    "        total_reward += 20 * 2 ** (line_diff)\n",
    "    \n",
    "    if include_holes:\n",
    "        total_reward -= hole_count * 1.5\n",
    "    if include_towers:\n",
    "        total_reward -= include_towers\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "def num_holes(state):\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    return np.sum(np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)[1:, :])\n",
    "\n",
    "def num_holy_towers(state):\n",
    "    \"\"\"This is a fucking work of art\"\"\"\n",
    "    flat_state = np.where(state.cpu() > 0, 1, 0).squeeze(0).squeeze(0)\n",
    "    mask = np.where((np.roll(flat_state, flat_state.shape[1]) > 0) & (flat_state == 0), 1, 0)\n",
    "    return np.sum(np.where(mask, flat_state.cumsum(axis=0), 0))\n",
    "\n",
    "def train(num_episodes = 1000, human=False): \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        height, lines = 0, 0\n",
    "        env.reset()\n",
    "        last_state = get_screen(human=human)\n",
    "        state = get_screen(human=human)\n",
    "        hole_count = 0 \n",
    "        hole_reward = 0\n",
    "        tower_count = 0 \n",
    "        tower_reward = 0\n",
    "        if not human:\n",
    "            state_array = [last_state] * MULTISTEP_PARAM\n",
    "            reward_array = [0] * MULTISTEP_PARAM\n",
    "            \n",
    "            reward_sum = 0\n",
    "            array_pos = 0\n",
    "            next_array_pos = 1\n",
    "            warmup = 1\n",
    "        for t in count():\n",
    "\n",
    "            # Select and perform an action\n",
    "            action = select_action(state, deterministic=human)\n",
    "            # Can only perform an action once every three frames anyway...\n",
    "            state, _, done, info = env.step(action.item())\n",
    "            piece_fell = did_piece_fall(env)\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "            if not done:\n",
    "                state, _, done, info = env.step(0)\n",
    "                piece_fell = (piece_fell or did_piece_fall(env))\n",
    "\n",
    "            # Observe new state\n",
    "            state = get_screen(state, human)\n",
    "            \n",
    "            if not human:\n",
    "                state_array[array_pos] = state\n",
    "                \n",
    "                # Rewards\n",
    "                if piece_fell:\n",
    "                    # Holes\n",
    "                    new_holes = num_holes(last_state)\n",
    "                    holes_reward = new_holes - hole_count\n",
    "                    hole_count = new_holes\n",
    "                    # Towers\n",
    "                    new_towers = num_holy_towers(last_state)\n",
    "                    tower_reward = new_towers - tower_count\n",
    "                    tower_count = new_towers\n",
    "                else:\n",
    "                    holes_reward = 0\n",
    "                    tower_reward = 0\n",
    "                    \n",
    "                reward_single = create_reward(env, piece_fell, action, done, info, height, lines, holes_reward, tower_reward)\n",
    "                reward_sum = (MULISTEP_GAMMA * reward_sum) + reward_single - (MULISTEP_GAMMA ** MULTISTEP_PARAM) * reward_array[array_pos]\n",
    "                reward_array[array_pos] = reward_single\n",
    "                reward_sum = torch.tensor([reward_sum], device=device).type(torch.float)\n",
    "                \n",
    "                # Store the transition in memory\n",
    "                if warmup > MULTISTEP_PARAM:\n",
    "#                     with torch.no_grad():\n",
    "#                         loss = compute_loss_single(state_array[next_array_pos], action, state, reward_sum) ** ((1 - curr_eps(steps_done)) / 2 + 0.05)\n",
    "#                     memory.push(state_array[next_array_pos], action, state, reward_sum, bias=np.array([loss.cpu()])[0])\n",
    "                    memory.push(state_array[next_array_pos], action, state, reward_sum)\n",
    "                \n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                if (warmup + 1) % TRAIN_RATE == 0:\n",
    "                    optimize_model()\n",
    "                if done:\n",
    "                    episode_durations.append(t + 1)\n",
    "                    lines_cleared.append(lines)\n",
    "                    eps_values.append(curr_eps(steps_done))\n",
    "                    plot_durations('latest.png')\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Set up params for next cycle\n",
    "            height = info['height']\n",
    "            lines = env.unwrapped.game.complete_lines\n",
    "            last_state = state\n",
    "            if not human:\n",
    "                array_pos = (array_pos + 1) % MULTISTEP_PARAM\n",
    "                next_array_pos = (next_array_pos + 1) % MULTISTEP_PARAM\n",
    "                warmup += 1\n",
    "            \n",
    "        if not human:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "def watch_model(rounds=1000):\n",
    "    with torch.no_grad():\n",
    "        train(rounds, human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7ad6f0529ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{load_net_prefix}{idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-549ee3c84174>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwarmup\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTRAIN_RATE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mepisode_durations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-950178303389>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Compute Huber loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-950178303389>\u001b[0m in \u001b[0;36m_compute_loss\u001b[0;34m(_state, _action, _next_state, _reward, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# columns of actions taken. These are the actions which would've been taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# for each batch state according to policy_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Compute V(s_{t+1}) for all next states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7e9c7624fe44>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Called with either one element to determine next action, or a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = 14\n",
    "while True:\n",
    "    train(5000)\n",
    "    torch.save(policy_net, f'{load_net_prefix}{idx}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, f'{load_net_prefix}{idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gym.envs.classic_control.rendering.SimpleImageViewer object at 0x7fef7f47db38>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ae85c8d89772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-72bf5be30025>\u001b[0m in \u001b[0;36mwatch_model\u001b[0;34m(rounds)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwatch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-72bf5be30025>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, human)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Observe new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m(screen, human)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Turn greyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Compress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ed0e47bc432c>\u001b[0m in \u001b[0;36mclean_state\u001b[0;34m(state_var)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Returns a greyscale image with pixels taking values in [0,1]. Also adds a batch dimension\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgreyscale\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgreyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2076\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "watch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0765313099202285"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_eps(steps_done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
